{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook focus at first explaining basic layers used in Keras , its' few key parameters, then it's applicability to regression problem on online news popularity dataset downloaded from UCI data repo\n",
    "\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/wine/\n",
    "\n",
    "Abstract: Using chemical analysis determine the origin of wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Reshape, Concatenate, Dense, Activation, Dropout\n",
    "from keras.models import load_model,Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam,RMSprop,SGD\n",
    "from keras.datasets import mnist\n",
    "from keras.wrappers.scikit_learn import KerasRegressor, KerasClassifier\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_yaml, save_model\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import SVG\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   name  alcohol  malicAcid   ash  ashalcalinity  magnesium  totalPhenols  \\\n",
      "0     1    14.23       1.71  2.43           15.6        127          2.80   \n",
      "1     1    13.20       1.78  2.14           11.2        100          2.65   \n",
      "2     1    13.16       2.36  2.67           18.6        101          2.80   \n",
      "3     1    14.37       1.95  2.50           16.8        113          3.85   \n",
      "4     1    13.24       2.59  2.87           21.0        118          2.80   \n",
      "\n",
      "   flavanoids  nonFlavanoidPhenols  proanthocyanins  colorIntensity   hue  \\\n",
      "0        3.06                 0.28             2.29            5.64  1.04   \n",
      "1        2.76                 0.26             1.28            4.38  1.05   \n",
      "2        3.24                 0.30             2.81            5.68  1.03   \n",
      "3        3.49                 0.24             2.18            7.80  0.86   \n",
      "4        2.69                 0.39             1.82            4.32  1.04   \n",
      "\n",
      "   od280_od315  proline  \n",
      "0         3.92     1065  \n",
      "1         3.40     1050  \n",
      "2         3.17     1185  \n",
      "3         3.45     1480  \n",
      "4         2.93      735  \n"
     ]
    }
   ],
   "source": [
    "project='Wine-source'\n",
    "datapath = os.path.join('D:','\\Learning','General','data',project)\n",
    "data = pd.read_csv(os.path.join(datapath,'wine.data'),header=None)\n",
    "data.columns = [  'name'\n",
    "                 ,'alcohol',\n",
    "                'malicAcid',\n",
    "                'ash'\n",
    "                ,'ashalcalinity'\n",
    "                ,'magnesium'\n",
    "                ,'totalPhenols'\n",
    "                ,'flavanoids'\n",
    "                ,'nonFlavanoidPhenols'\n",
    "                ,'proanthocyanins'\n",
    "                ,'colorIntensity'\n",
    "                ,'hue'\n",
    "                ,'od280_od315'\n",
    "                ,'proline'\n",
    "                ]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a small sample for quick run\n",
    "# data=data.sample(frac=0.5)\n",
    "X = data.loc[:, data.columns != 'name']\n",
    "y = data['name']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "# convert integers to one hot encoded\n",
    "one_hot_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# encoder.fit(y_test)\n",
    "# encoded_Ytest = encoder.transform(y_test)\n",
    "# # convert integers to one hot encoded\n",
    "# one_hot_ytest = np_utils.to_categorical(encoded_Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying Input shape : \n",
    "First layer need to tell about the input shape , following layers can do the automatic shape inference\n",
    "Important layers to consider are \n",
    "a) Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n",
    "    units: Positive integer, dimensionality of the output space.\n",
    "    activation: Activation function to use. Nothing specified, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "    use_bias: Boolean, whether the layer uses a bias vector\n",
    "     & many more parameters\n",
    "\n",
    "You can define model architecture in 2 ways :\n",
    "1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras input layers \n",
    "## continuous variable input\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "# now the model will take as input arrays of shape (*, 100)\n",
    "# and output arrays of shape (*, 64)\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Dense(64, input_shape=(100,)),\n",
    "                    Activation('relu'),\n",
    "                    Dense(10),\n",
    "                    Activation('softmax'),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both model arch 1 and 2 are same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizer\n",
    "##### Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes.\n",
    "##### L1 /L2 and also dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 /L2 example\n",
    "model.add(Dense(64, input_dim=64,kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.01)))\n",
    "# Dropout example \n",
    "model.add(Dropout(rate=0.1, noise_shape=None, seed=3))\n",
    "# where rate is dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In compilation various loss functions, optimizer and metrics can be defined . Few of them are\n",
    "##### Optimizers : sgd , rmsprop, adam, adagrad, adadelta, adamax, nadam\n",
    "##### Loss: categorical_crossentropy for muti class problem\n",
    "#####            mean_squared_error, mean_absolute_error, mean_absolute_percentage_error,mean_squared_logarithmic_error for continous variable\n",
    "#####            binary_crossentropy for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "# if you want to train on batch input\n",
    "model.train_on_batch(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application on online news popularity dataset (Regression problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model with first layer being input and output same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 50\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=5, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "94/94 [==============================] - 1s 14ms/step - loss: 11.6599 - acc: 0.2766\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 0s 774us/step - loss: 11.6599 - acc: 0.2766\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 0s 753us/step - loss: 11.6599 - acc: 0.2766\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 0s 775us/step - loss: 11.6599 - acc: 0.2766\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 0s 732us/step - loss: 11.6599 - acc: 0.2766\n",
      "48/48 [==============================] - 0s 9ms/step\n",
      "Epoch 1/5\n",
      "95/95 [==============================] - 1s 14ms/step - loss: 11.3675 - acc: 0.2947\n",
      "Epoch 2/5\n",
      "95/95 [==============================] - 0s 808us/step - loss: 11.3675 - acc: 0.2947\n",
      "Epoch 3/5\n",
      "95/95 [==============================] - 0s 745us/step - loss: 11.3675 - acc: 0.2947\n",
      "Epoch 4/5\n",
      "95/95 [==============================] - 0s 734us/step - loss: 11.3675 - acc: 0.2947\n",
      "Epoch 5/5\n",
      "95/95 [==============================] - 0s 756us/step - loss: 11.3675 - acc: 0.2947\n",
      "47/47 [==============================] - 0s 10ms/step\n",
      "Epoch 1/5\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 11.1978 - acc: 0.3053\n",
      "Epoch 2/5\n",
      "95/95 [==============================] - 0s 777us/step - loss: 11.1978 - acc: 0.3053\n",
      "Epoch 3/5\n",
      "95/95 [==============================] - 0s 672us/step - loss: 11.1978 - acc: 0.3053\n",
      "Epoch 4/5\n",
      "95/95 [==============================] - 0s 745us/step - loss: 11.1978 - acc: 0.3053\n",
      "Epoch 5/5\n",
      "95/95 [==============================] - 0s 745us/step - loss: 11.1978 - acc: 0.3053\n",
      "47/47 [==============================] - 0s 10ms/step\n",
      "Results: 0.35 (0.06) Accuracy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, X_train, one_hot_y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) Accuracy\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # evaluate model with standardized dataset\n",
    "# estimators = []\n",
    "# # estimators.append(('standardize', StandardScaler()))\n",
    "# estimators.append(('classify', KerasRegressor(build_fn=baseline_model, epochs=5, batch_size=5, verbose=1)))\n",
    "# pipeline = Pipeline(estimators)\n",
    "# kfold = KFold(n_splits=3,shuffle=True, random_state=seed)\n",
    "# results = cross_val_score(pipeline, X_train, one_hot_y, cv=kfold)\n",
    "# print(\"Standardized: %.2f (%.2f) Accuracy\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try deeper model with 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def deeper_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(3,activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "94/94 [==============================] - 2s 19ms/step - loss: 8.4365 - acc: 0.3617\n",
      "Epoch 2/5\n",
      "94/94 [==============================] - 0s 265us/step - loss: 8.8424 - acc: 0.3511\n",
      "Epoch 3/5\n",
      "94/94 [==============================] - 0s 276us/step - loss: 9.3831 - acc: 0.2766\n",
      "Epoch 4/5\n",
      "94/94 [==============================] - 0s 307us/step - loss: 7.7160 - acc: 0.4043\n",
      "Epoch 5/5\n",
      "94/94 [==============================] - 0s 297us/step - loss: 9.0944 - acc: 0.3511\n",
      "48/48 [==============================] - 1s 16ms/step\n",
      "Epoch 1/5\n",
      "95/95 [==============================] - 2s 19ms/step - loss: 9.8704 - acc: 0.3579\n",
      "Epoch 2/5\n",
      "95/95 [==============================] - 0s 357us/step - loss: 9.5976 - acc: 0.3684\n",
      "Epoch 3/5\n",
      "95/95 [==============================] - 0s 346us/step - loss: 9.2889 - acc: 0.3789\n",
      "Epoch 4/5\n",
      "95/95 [==============================] - 0s 346us/step - loss: 9.0706 - acc: 0.4105\n",
      "Epoch 5/5\n",
      "95/95 [==============================] - 0s 325us/step - loss: 8.2617 - acc: 0.4211\n",
      "47/47 [==============================] - 1s 14ms/step\n",
      "Epoch 1/5\n",
      "95/95 [==============================] - 2s 18ms/step - loss: 7.9895 - acc: 0.4526\n",
      "Epoch 2/5\n",
      "95/95 [==============================] - 0s 305us/step - loss: 8.4879 - acc: 0.3579\n",
      "Epoch 3/5\n",
      "95/95 [==============================] - 0s 304us/step - loss: 9.8034 - acc: 0.3474\n",
      "Epoch 4/5\n",
      "95/95 [==============================] - 0s 294us/step - loss: 7.7473 - acc: 0.4421\n",
      "Epoch 5/5\n",
      "95/95 [==============================] - 0s 262us/step - loss: 8.5815 - acc: 0.4316\n",
      "47/47 [==============================] - 1s 14ms/step\n",
      "[-11.41698416 -11.31696063  -5.19003553]\n",
      "Deeper model: -9.31 (2.91) Accuracy\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "# estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('classify', KerasRegressor(build_fn=deeper_model, epochs=5, batch_size=20, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=3,shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_train, one_hot_y, cv=kfold)\n",
    "print(results)\n",
    "print(\"Deeper model: %.2f (%.2f) Accuracy\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "142/142 [==============================] - 2s 13ms/step - loss: 11.3508 - acc: 0.2958\n",
      "Epoch 2/5\n",
      "142/142 [==============================] - 0s 772us/step - loss: 11.3508 - acc: 0.2958\n",
      "Epoch 3/5\n",
      "142/142 [==============================] - 0s 752us/step - loss: 11.3508 - acc: 0.2958\n",
      "Epoch 4/5\n",
      "142/142 [==============================] - 0s 759us/step - loss: 11.3508 - acc: 0.2958\n",
      "Epoch 5/5\n",
      "142/142 [==============================] - 0s 751us/step - loss: 11.3508 - acc: 0.2958\n",
      "36/36 [==============================] - 1s 19ms/step\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "36.11111111111111\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(X_train, one_hot_y)\n",
    "prediction = estimator.predict(X_test)\n",
    "print(prediction)\n",
    "print(encoder.inverse_transform(prediction))\n",
    "print(mean_absolute_percentage_error(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your NN model in YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to directory\n"
     ]
    }
   ],
   "source": [
    "# save model to yaml\n",
    "model_yaml = estimator.model.to_yaml()\n",
    "with open(os.path.join(datapath,\"output\",\"model.yaml\"), \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# save weights to HDF5\n",
    "estimator.model.save_weights(os.path.join(datapath,\"output\",\"model.h5\"))\n",
    "print(\"Saved model to directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Load YAML and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "yaml_file = open(os.path.join(datapath,\"output\",\"model.yaml\"), 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(os.path.join(datapath,\"output\",\"model.h5\"))\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate loaded model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3965/3965 [==============================] - 0s 121us/step\n",
      "mean_absolute_percentage_error: 66.21%\n"
     ]
    }
   ],
   "source": [
    "loaded_model.compile(loss='mean_absolute_percentage_error', optimizer='adam', metrics=['mean_absolute_percentage_error'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
