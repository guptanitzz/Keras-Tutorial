{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook focus at first explaining basic layers used in Keras , its' few key parameters, then it's applicability to regression problem on online news popularity dataset downloaded from UCI data repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Reshape, Concatenate, Dense, Activation, Dropout\n",
    "from keras.models import load_model,Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from IPython.display import SVG\n",
    "from keras.optimizers import Adam,RMSprop,SGD\n",
    "from keras.datasets import mnist\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project='OnlineNewsPopularity'\n",
    "datapath=os.path.join('D:','\\Learning','General','data',project)\n",
    "data = pd.read_csv(os.path.join(datapath,'OnlineNewsPopularity.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a small sample for quick run\n",
    "data=data.sample(frac=0.5)\n",
    "X = data.select_dtypes(include=np.number)\n",
    "y = data[' shares']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying Input shape : \n",
    "First layer need to tell about the input shape , following layers can do the automatic shape inference\n",
    "Important layers to consider are \n",
    "a) Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n",
    "    units: Positive integer, dimensionality of the output space.\n",
    "    activation: Activation function to use. Nothing specified, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "    use_bias: Boolean, whether the layer uses a bias vector\n",
    "     & many more parameters\n",
    "\n",
    "You can define model architecture in 2 ways :\n",
    "1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras input layers \n",
    "## continuous variable input\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "# now the model will take as input arrays of shape (*, 100)\n",
    "# and output arrays of shape (*, 64)\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Dense(64, input_shape=(100,)),\n",
    "                    Activation('relu'),\n",
    "                    Dense(10),\n",
    "                    Activation('softmax'),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both model arch 1 and 2 are same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizer\n",
    "##### Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes.\n",
    "##### L1 /L2 and also dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 /L2 example\n",
    "model.add(Dense(64, input_dim=64,kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.01)))\n",
    "# Dropout example \n",
    "model.add(Dropout(rate=0.1, noise_shape=None, seed=3))\n",
    "# where rate is dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In compilation various loss functions, optimizer and metrics can be defined . Few of them are\n",
    "##### Optimizers : sgd , rmsprop, adam, adagrad, adadelta, adamax, nadam\n",
    "##### Loss: categorical_crossentropy for muti class problem\n",
    "#####            mean_squared_error, mean_absolute_error, mean_absolute_percentage_error,mean_squared_logarithmic_error for continous variable\n",
    "#####            binary_crossentropy for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "# if you want to train on batch input\n",
    "model.train_on_batch(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application on online news popularity dataset (Regression problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model with first layer being input and output same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 50\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=20, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('regress', KerasRegressor(build_fn=baseline_model, epochs=20, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_train, y_train, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try deeper model with 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def deeper_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(36, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('regress', KerasRegressor(build_fn=deeper_model, epochs=20, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Deeper model: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your NN model in YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to yaml\n",
    "model_yaml = model.to_yaml()\n",
    "with open(os.path.join(datapath,\"output\",\"model.yaml\"), \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# save weights to HDF5\n",
    "model.save_weights(os.path.join(datapath,\"output\",\"model.h5\")\n",
    "print(\"Saved model to directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Load YAML and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = open((os.path.join(datapath,\"output\",\"model.yaml\"), 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights((os.path.join(datapath,\"output\",\"model.h5\"))\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate loaded model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
