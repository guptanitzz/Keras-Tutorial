{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook focus at first explaining basic layers used in Keras , its' few key parameters, then it's applicability to regression problem on online news popularity dataset downloaded from UCI data repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Reshape, Concatenate, Dense, Activation, Dropout\n",
    "from keras.models import load_model,Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from IPython.display import SVG\n",
    "from keras.optimizers import Adam,RMSprop,SGD\n",
    "from keras.datasets import mnist\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_yaml, save_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "from sklearn import preprocessing, svm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.utils import check_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = check_array(y_true, y_pred)\n",
    "    ## Note: does not handle mix 1d representation\n",
    "    if _is_1d(y_true): \n",
    "        y_true, y_pred = _check_1d_array(y_true, y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project='OnlineNewsPopularity'\n",
    "datapath=os.path.join('D:','\\Learning','General','data',project)\n",
    "data = pd.read_csv(os.path.join(datapath,'OnlineNewsPopularity.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a small sample for quick run\n",
    "data=data.sample(frac=0.5)\n",
    "X = data.select_dtypes(include=np.number)\n",
    "y = data[' shares']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying Input shape : \n",
    "First layer need to tell about the input shape , following layers can do the automatic shape inference\n",
    "Important layers to consider are \n",
    "a) Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n",
    "    units: Positive integer, dimensionality of the output space.\n",
    "    activation: Activation function to use. Nothing specified, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "    use_bias: Boolean, whether the layer uses a bias vector\n",
    "     & many more parameters\n",
    "\n",
    "You can define model architecture in 2 ways :\n",
    "1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras input layers \n",
    "## continuous variable input\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
    "# now the model will take as input arrays of shape (*, 100)\n",
    "# and output arrays of shape (*, 64)\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Dense(64, input_shape=(100,)),\n",
    "                    Activation('relu'),\n",
    "                    Dense(10),\n",
    "                    Activation('softmax'),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both model arch 1 and 2 are same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizer\n",
    "##### Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes.\n",
    "##### L1 /L2 and also dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 /L2 example\n",
    "model.add(Dense(64, input_dim=64,kernel_regularizer=regularizers.l2(0.01),activity_regularizer=regularizers.l1(0.01)))\n",
    "# Dropout example \n",
    "model.add(Dropout(rate=0.1, noise_shape=None, seed=3))\n",
    "# where rate is dropout rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In compilation various loss functions, optimizer and metrics can be defined . Few of them are\n",
    "##### Optimizers : sgd , rmsprop, adam, adagrad, adadelta, adamax, nadam\n",
    "##### Loss: categorical_crossentropy for muti class problem\n",
    "#####            mean_squared_error, mean_absolute_error, mean_absolute_percentage_error,mean_squared_logarithmic_error for continous variable\n",
    "#####            binary_crossentropy for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train and y_train are Numpy arrays --just like in the Scikit-Learn API.\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "# if you want to train on batch input\n",
    "model.train_on_batch(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application on online news popularity dataset (Regression problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model with first layer being input and output same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 50\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_absolute_percentage_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=5, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10571/10571 [==============================] - 5s 498us/step - loss: 95.2058\n",
      "Epoch 2/5\n",
      "10571/10571 [==============================] - 5s 479us/step - loss: 216.4079\n",
      "Epoch 3/5\n",
      "10571/10571 [==============================] - 5s 482us/step - loss: 131.3182\n",
      "Epoch 4/5\n",
      "10571/10571 [==============================] - 5s 477us/step - loss: 125.1746\n",
      "Epoch 5/5\n",
      "10571/10571 [==============================] - 5s 483us/step - loss: 225.9808\n",
      "5286/5286 [==============================] - 1s 170us/step\n",
      "Epoch 1/5\n",
      "10571/10571 [==============================] - 5s 502us/step - loss: 169.5844\n",
      "Epoch 2/5\n",
      "10571/10571 [==============================] - 5s 501us/step - loss: 287.1242\n",
      "Epoch 3/5\n",
      "10571/10571 [==============================] - 5s 497us/step - loss: 257.8465\n",
      "Epoch 4/5\n",
      "10571/10571 [==============================] - 5s 496us/step - loss: 159.1571\n",
      "Epoch 5/5\n",
      "10571/10571 [==============================] - 5s 492us/step - loss: 357.8413\n",
      "5286/5286 [==============================] - 1s 173us/step\n",
      "Epoch 1/5\n",
      "10572/10572 [==============================] - 5s 510us/step - loss: 121.6339\n",
      "Epoch 2/5\n",
      "10572/10572 [==============================] - 5s 511us/step - loss: 79.8756\n",
      "Epoch 3/5\n",
      "10572/10572 [==============================] - 5s 505us/step - loss: 89.5460\n",
      "Epoch 4/5\n",
      "10572/10572 [==============================] - 5s 499us/step - loss: 66.2120\n",
      "Epoch 5/5\n",
      "10572/10572 [==============================] - 5s 490us/step - loss: 36.4231\n",
      "5285/5285 [==============================] - 1s 177us/step\n",
      "Results: -20.81 (6.52) MSE\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=3, random_state=seed)\n",
    "results = cross_val_score(estimator, X_train, y_train, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10571/10571 [==============================] - 6s 560us/step - loss: 90.1253\n",
      "Epoch 2/5\n",
      "10571/10571 [==============================] - 5s 495us/step - loss: 74.6968\n",
      "Epoch 3/5\n",
      "10571/10571 [==============================] - 5s 506us/step - loss: 68.1048\n",
      "Epoch 4/5\n",
      "10571/10571 [==============================] - 6s 525us/step - loss: 64.4174\n",
      "Epoch 5/5\n",
      "10571/10571 [==============================] - 6s 539us/step - loss: 61.9058\n",
      "5286/5286 [==============================] - 1s 181us/step\n",
      "Epoch 1/5\n",
      "10571/10571 [==============================] - 6s 529us/step - loss: 87.2120\n",
      "Epoch 2/5\n",
      "10571/10571 [==============================] - 5s 505us/step - loss: 70.5276\n",
      "Epoch 3/5\n",
      "10571/10571 [==============================] - 5s 512us/step - loss: 65.5541\n",
      "Epoch 4/5\n",
      "10571/10571 [==============================] - 5s 515us/step - loss: 61.9389\n",
      "Epoch 5/5\n",
      "10571/10571 [==============================] - 5s 497us/step - loss: 59.2776\n",
      "5286/5286 [==============================] - 1s 179us/step\n",
      "Epoch 1/5\n",
      "10572/10572 [==============================] - 5s 507us/step - loss: 91.0420\n",
      "Epoch 2/5\n",
      "10572/10572 [==============================] - 5s 480us/step - loss: 72.1251\n",
      "Epoch 3/5\n",
      "10572/10572 [==============================] - 6s 585us/step - loss: 65.2585\n",
      "Epoch 4/5\n",
      "10572/10572 [==============================] - 5s 497us/step - loss: 61.3667\n",
      "Epoch 5/5\n",
      "10572/10572 [==============================] - 5s 495us/step - loss: 58.2720\n",
      "5285/5285 [==============================] - 1s 180us/step\n",
      "Standardized: -68.63 (5.69) MSE\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('regress', KerasRegressor(build_fn=baseline_model, epochs=5, batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=3, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_train, y_train, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try deeper model with 4 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "def deeper_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(36, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_absolute_percentage_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10571/10571 [==============================] - 9s 809us/step - loss: 70.8818\n",
      "Epoch 2/5\n",
      "10571/10571 [==============================] - 8s 779us/step - loss: 59.0442\n",
      "Epoch 3/5\n",
      "10571/10571 [==============================] - 8s 777us/step - loss: 62.2299\n",
      "Epoch 4/5\n",
      "10571/10571 [==============================] - 8s 768us/step - loss: 56.1285\n",
      "Epoch 5/5\n",
      "10571/10571 [==============================] - 8s 792us/step - loss: 62.0797\n",
      "5286/5286 [==============================] - 1s 233us/step\n",
      "Epoch 1/5\n",
      "10571/10571 [==============================] - 9s 874us/step - loss: 63.8767\n",
      "Epoch 2/5\n",
      "10571/10571 [==============================] - 9s 833us/step - loss: 62.5697\n",
      "Epoch 3/5\n",
      "10571/10571 [==============================] - 9s 807us/step - loss: 63.0693\n",
      "Epoch 4/5\n",
      "10571/10571 [==============================] - 9s 811us/step - loss: 57.2315\n",
      "Epoch 5/5\n",
      "10571/10571 [==============================] - 9s 808us/step - loss: 57.6447\n",
      "5286/5286 [==============================] - 1s 251us/step\n",
      "Epoch 1/5\n",
      "10572/10572 [==============================] - 9s 868us/step - loss: 65.0386\n",
      "Epoch 2/5\n",
      "10572/10572 [==============================] - 9s 827us/step - loss: 56.1675\n",
      "Epoch 3/5\n",
      "10572/10572 [==============================] - 9s 831us/step - loss: 54.8803\n",
      "Epoch 4/5\n",
      "10572/10572 [==============================] - 9s 817us/step - loss: 53.9118\n",
      "Epoch 5/5\n",
      "10572/10572 [==============================] - 9s 808us/step - loss: 51.7767\n",
      "5285/5285 [==============================] - 1s 233us/step\n",
      "Deeper model: -65.02 (11.46) MSE\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('regress', KerasRegressor(build_fn=deeper_model, epochs=5, batch_size=5, verbose=1)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=3, random_state=seed)\n",
    "results = cross_val_score(pipeline, X_train, y_train, cv=kfold)\n",
    "print(\"Deeper model: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "15857/15857 [==============================] - 9s 568us/step - loss: 133.7102\n",
      "Epoch 2/5\n",
      "15857/15857 [==============================] - 9s 537us/step - loss: 120.7474\n",
      "Epoch 3/5\n",
      "15857/15857 [==============================] - 8s 526us/step - loss: 135.2436\n",
      "Epoch 4/5\n",
      "15857/15857 [==============================] - 8s 500us/step - loss: 138.4392\n",
      "Epoch 5/5\n",
      "15857/15857 [==============================] - 8s 526us/step - loss: 170.4065\n",
      "3965/3965 [==============================] - 1s 182us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'check_arrays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-aa93b8067b9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-49e05cd44942>\u001b[0m in \u001b[0;36mmean_absolute_percentage_error\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m## Note: does not handle mix 1d representation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_1d_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'check_arrays' is not defined"
     ]
    }
   ],
   "source": [
    "estimator.fit(X_train, y_train)\n",
    "prediction = estimator.predict(X_test)\n",
    "print(mean_absolute_percentage_error(y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your NN model in YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasRegressor' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-41bba1bcbeb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# save model to yaml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_yaml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'regress'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_yaml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatapath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"output\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"model.yaml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0myaml_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0myaml_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_yaml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# save weights to HDF5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KerasRegressor' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "# save model to yaml\n",
    "model_yaml = pipeline.named_steps['regress'].model.to_yaml()\n",
    "with open(os.path.join(datapath,\"output\",\"model.yaml\"), \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "# save weights to HDF5\n",
    "model.save_weights(os.path.join(datapath,\"output\",\"model.h5\"))\n",
    "print(\"Saved model to directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Load YAML and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = open((os.path.join(datapath,\"output\",\"model.yaml\"), 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights((os.path.join(datapath,\"output\",\"model.h5\"))\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate loaded model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
